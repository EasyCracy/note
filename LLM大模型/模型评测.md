## 介绍
### 什么是模型评测

模型评测（Model Evaluation）是指对人工智能或机器学习模型的性能进行系统性**测量**和**分析**的过程。评测的目标是衡量模型的准确性、效率、鲁棒性、公平性等量化指标，以确保其在实际应用中达到预期结果。
常见的评测指标包括**准确率**、**精确率**、**召回率**、**F1-score**、**AUC-ROC**，以及针对模型的**困惑度**、**多样性**、**推理速度**等。此外，大模型评测还可能涉及**对抗性**、**偏见检测**以及**人类偏好对齐**（RRLHF）等方面

**鲁棒性**：类似于软件测试中的<mark>健壮性</mark>，遇到异常时系统仍能保持正常的运行，视为抗干扰或抗故障的能力。

### 评测维度

当前市场上有文本模型、多模态模型、音视频模型、以及其它专业领域模型。每个大模型的侧重点不一。

**文本模型**：编码能力、推理能力、写作能力
**多模态模型**：视觉理解、图像生成、视频生成
**专业领域模型**：金融、法律、医疗等，专业领域的深度与准确性要求更高

---
### 评测方法

**自动评测**：自定义的评测指标，使用一个模型来评估另一个模型的性能，该方式具有高效、公正以及节省成本和时间的优势，但评测结果高度依赖人为设定的评分维度和标准。该方法适用于在特定业务场景下进行模型比选

**基线评测**：通过预制的基线评测集对模型的各项基础能力进行评测，如GSM8K，C-Eval，MMLU、MMBench、HumanEval等主流公开数据集，该方法适用于对微调模型的基本效果进行评价，以避免模型的通用泛化能力发生明显下降。

**人工评估**：由专家或标注员对模型输出在有用性、安全性等方面进行打分。

| 数据集名称     | 数据集描述                  |
| --------- | ---------------------- |
| MMLU      | 评估模型世界知识掌握情况和解决问题的能力   |
| C-Eval    | 评估模型对中文文本的理解和应用能力      |
| GSM8K     | 评估基础数学范畴内的多步推理和解决问题的能力 |
| MMBench   | 评估模型对图文和视频的理解能力        |
| HumanEval | 评估模型代码生成的编码能力          |
#### 数据集获取

模型训练的数据集可通过下列方法获取
1）公共数据集，如**Kaggle**社区，**CAMEL**对话数据集
2）互联网抓取，通过爬虫的形式抓取网络上公开的资源，但资源好坏不一，且有法律合规风险
3）使用现有生产线上数据，自己规范总结的数据

数据获取完成后并不能直接使用，还需要进行数据清洗
- 过滤掉广告、垃圾信息等干扰内容。
- 删除无关或重复的内容。
- 统一文本编码和格式

---
### 模型微调

模型微调是通过微调工具，使用独特的场景数据对平台的基础模型进行调整，帮助你快速定制一个更符合业务需求的大型模型。
进行模型微调需要自行准备训练数据，微调训练数据通常由一批包含输入和预期输出的数据组成，每条训练数据包含一个输入（Prompt）及其对应的预期输出。

